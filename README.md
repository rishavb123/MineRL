## Using Machine Learning to Train an Agent to Terminate Mobs in Minecraft

By Ahsan, F., Bhagat, R., Korunda, D., Moorman, J., & Moradia, S.

### Abstract

We will be using the popular sandbox video game Minecraft and its associated Malmo platform to train an agent to successfully combat in-game entities known as “mobs.” It is our hope that methods like the ones we are using to train an agent in a simulated environment can be extrapolated to real life applications like robotics in the physical world. We will use a popular reinforcement learning technique called Deep Q Learning using a couple different feature representations and compare the differences in performances. 

### Introduction and Background

Minecraft is a popular sandbox video game that contains a number of hostile non-player entities known as “mobs”; these entities are meant to attack and kill the player character. Our agent will have to learn strategies to deal with each type of hostile mob with the goal of defeating as many mobs and surviving as long as possible. Additionally, the environment in a Minecraft “world” can be randomly generated using an algorithm or built by the player. To create a closed environment for our agent to learn and fight against these mobs, we will be using Microsoft’s Project Malmo. Using machine learning in Minecraft is the focus of a large competition called MineRL, which provides rigorous guidelines towards achieving an agent that can operate autonomously in Minecraft.

### Problem Definition

The agent will have to last as long as possible while defeating as many distinct hostile entities as possible and navigating the environment. The agent will receive positive rewards for defeating entities/surviving and negative rewards for being defeated or losing health itself. We would ideally like to define a dense reward system, but if this does not prove to be possible, we will stick with a sparse reward system, such as death equating to a large negative reward.

### Methods

We are planning on using a Convolutional Deep Q Network to take in the image input and output what action(s) to take. Similarly to most CNNs, we will start with the CNN workflow (Convolution, Max Pooling, Activation) and then use some fully connected layers. We will also use a replay buffer to allow the agent to have “memory,” giving the agent a way to perceive motion between frames. Another implementation detail is that we will use a target network that we will copy the weights to periodically so that our DQN converges to a more stable solution. As some research showed us, using a recurrent neural network will not give us significant improvements so this is not a path we will follow[2].

A baseline model we can use would be taking the feature representation from a large pre trained CNN such as ResNet50, by using the model and excluding the final dense layer, and using this in place of our convolution layers. This would likely get us some performance, but will inherently be worse, since we have fixed some of our trainable parameters.

If we have extra time, we would also like to see the performance of using the lower dimensional space generated by an autoencoder as our input for the DQN instead of the representation from ResNet50 or the image itself. We can create an autoencoder by creating a dataset from the images we get from our environment and then using an encoder and decoder in series to approximate the original image. Then using an optimizer to reduce the MSE between the generated image and the original image should yield us both an encoder and decoder.

### Potential Results

Early on, we would expect a low number of kills or hits per lifetime for the agent. From experience, it is very easy to become overwhelmed by different types of mobs and considering the agent will not be fighting them one at a time, we would expect a number of kills from the single digits to the low teens. We believe in the same light, we’d see an initial low time alive, but slowly see an increase occur, in a curve that may mimic a logistic function in shape [2]. 

We also expect including the CNN to perform better than using ResNet50 or an autoencoder since it gives the DQN the most freedom when it comes to processing the image.

### Discussion / Conclusion

We have learned about the formulation and rudimentary training process for Q-learning and Deep Q Networks. We have deliberated between the use of continuous and discrete action spaces, as well as the size of the action space. We have explored this scenario in terms of a Markov Decision Process and we have likened it to a robotics navigation task. We hope to learn to implement a Deep Q Network including a CNN as well as possibly other RL architectures for a simulated discrete environment. We also hope to learn broadly about RL algorithms in general and in which instances some would be preferable to others and why this is the case.

### References
[1] Christian S., Yanick S., & Manfred V. (2020). Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft. arXiv. Retrieved March 1, 2021, from https://arxiv.org/abs/2003.06066

[2] Clément R., & Vincent B. (2019). Deep Recurrent Q-Learning vs Deep Q-Learning on a simple Partially Observable Markov Decision Process with Minecraft. arXiv. Retrieved March 1, 2021, from https://arxiv.org/abs/1903.04311

[3] Volodymyr M., Koray, K., David, S., Alex, G., Ioannis A., Daan W., & Martin R. (2013). Playing Atari with Deep Reinforcement Learning. arXiv. Retrieved March 1, 2021, from https://arxiv.org/abs/1312.5602